{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HiC-Pro \n",
    "def preparing_HiCPro_file(h3dgipdir, genome, enzyme, input_dir):\n",
    "    import os\n",
    "    output_file = \"./HiCPro/run_HiC-Pro.sh\"\n",
    "    fp = open(output_file, 'w')\n",
    "    # 1、Enzyme fragments\n",
    "    enzyme_fragments_order=\"python {0}/utils/digest_genome.py {1} -r {2} -o HiC-Pro_enzyme_fragments.bed\".format(h3dgipdir, genome, enzyme)\n",
    "    print(enzyme_fragments_order, file=fp)\n",
    "    # 2、Chromosome size\n",
    "    chromosome_size_step1 = \"samtools faidx {0} -o {0}.fai\".format(genome)\n",
    "    chromosome_size_step2 = \"awk -v OFS='\\\\t' \" + \" \\'{print $1, $2}\\' \" + \"{0}.fai > HiC-Pro_chromosome_size.txt\".format(genome)\n",
    "    print(chromosome_size_step1, file=fp)\n",
    "    print(chromosome_size_step2, file=fp)\n",
    "    # 3、genome index\n",
    "    ln_order = \"ln -s {}\".format(genome)\n",
    "    genomeName = genome.split(\"/\")[-1].split(\".\")[0]\n",
    "    build_index_order = \"bowtie2-build -f {0} --threads 4\".format(genomeName)\n",
    "    print(ln_order, file=fp)\n",
    "    print(build_index_order, file=fp)\n",
    "    # 4、change configure file\n",
    "    cp_order = \"cp {}/utils/HiC-Pro.config .\".format(h3dgipdir)\n",
    "    tmpDir = os.getcwd()\n",
    "    workDir = tmpDir.replace(\"/\", \"\\/\")\n",
    "    HiCProDir = \"{}\\/HiCPro\".format(workDir)\n",
    "    sed_order1 = \"sed -i 's/InderDir/{0}/g' HiC-Pro.config\".format(HiCProDir)\n",
    "    sed_order2 = \"sed -i 's/GenomeName/{0}/g' HiC-Pro.config\".format(genomeName)\n",
    "    sed_order3 = \"sed -i 's/ChromosomeSize/{0}\\/HiC-Pro_chromosome_size.txt/g' HiC-Pro.config\".format(HiCProDir)\n",
    "    ## HindIII\n",
    "    sed_order4 = \"sed -i 's/EnzymeFragment/{0}\\/HiC-Pro_enzyme_fragments.bed/g' HiC-Pro.config\".format(HiCProDir)\n",
    "    print(sed_order1, file=fp)\n",
    "    print(sed_order2, file=fp)\n",
    "    print(sed_order3, file=fp)\n",
    "    print(sed_order4, file=fp)\n",
    "    # 4 run HiC-Pro\n",
    "    HiCPro_step1 = \"HiC-Pro -c {0}/utils/HiC-Pro.config -i {1} -o HiCPro_result -p\".format(h3dgipdir, input_dir)\n",
    "    HiCPro_step2 = \"cd HiCPro_result\"\n",
    "    HiCPro_step3 = \"sh HiCPro_step1.sh\"\n",
    "    HiCPro_step4 = \"sh HiCPro_step2.sh\"\n",
    "    HiCPro_step5 = \"cd ..\"\n",
    "    print(HiCPro_step1, file=fp)\n",
    "    print(HiCPro_step2, file=fp)\n",
    "    print(HiCPro_step3, file=fp)\n",
    "    print(HiCPro_step4, file=fp)\n",
    "    print(HiCPro_step5, file=fp)\n",
    "    fp.close()\n",
    "\n",
    "# Juicer    \n",
    "def preparing_juicer_file(h3dgipdir, genome, enzyme, input_dir, compartment_resolution, tad_resolution, loop_resolution, name):\n",
    "    import os\n",
    "    output_file = \"juicer/run_juicer.sh\"\n",
    "    fp = open(output_file, \"w\")\n",
    "    genomeName = genome.split(\"/\")[-1].split(\".\")[0]\n",
    "    # 1、Enzyme fragm\n",
    "    enzyme_fragments_order='python {0}/utils/generate_site_positions.py {1} juicer_enzyme_fragments {2}'.format(h3dgipdir, enzyme, genome)\n",
    "    print(enzyme_fragments_order, file=fp)\n",
    "    # 2、Chromosome size\n",
    "    chromosome_size_step1 = \"samtools faidx {0} -o {0}.fai\".format(genome)\n",
    "    chromosome_size_step2 = \"awk -v OFS='\\\\t'\" + \" \\'{print $1, $2}\\' \" + \"{0}.fai > juicer_chromosome_size.txt\".format(genome)\n",
    "    print(chromosome_size_step1, file=fp)\n",
    "    print(chromosome_size_step2, file=fp)    \n",
    "    # 3、genome index\n",
    "    ln_order = \"ln -s {0} juicer/{0}\".format(genome)\n",
    "    build_index_order = \"bwa index {0}\".format(genome)\n",
    "    print(ln_order, file=fp)\n",
    "    print(build_index_order, file=fp)\n",
    "    # 4、Generates .hic file\n",
    "    workDir = os.getcwd()\n",
    "    genome_name = genome.split(\"/\")[-1].split(\".\")[0]\n",
    "    juicer_order = \"juicer.sh -z {0} -y {1}/juicer/juicer_enzyme_fragments_{2}.txt -p {1}/juicer/juicer_chromsome_size.txt -d {3} -D juicer_output/ -t 4\".format(genome, workDir, enzyme, input_dir)\n",
    "    print(juicer_order, file=fp)\n",
    "    # 5、divide Compartment\n",
    "    juicer_compartment = \"java -jar {0}/utils/juicer_scripts/juicer_tools.jar eigenvector KR juicer_output/{1}.hic chr1 BP {2}\".format(h3dgipdir, name, compartment_resolution)\n",
    "    print(juicer_compartment, file=fp)\n",
    "    print(\"#divide chr1 compartment\", file=fp)\n",
    "    # 6、Identify TADs\n",
    "    juicer_TAD = \"java -jar {0}/utils/juicer_scripts/juicer_tools.jar arrowhead -m 2000 -r {1} -k KR --threads 4 juicer_output/{2}.hic juicer_output\".format(h3dgipdir, tad_resolution, name)\n",
    "    print(juicer_TAD, file=fp)\n",
    "    # 7、Infer loops\n",
    "    juicer_loop = \"java -jar {0}/utils/juicer_scripts/juicer_tools.jar hiccups --cpu -r {1} -f 0.1 -p 4 -i 7 -d 20000 -t 0.02,1.5,1.75,2 -k KR --threads 4  juicer_output/{2}.hic juicer_output\".format(h3dgipdir, loop_resolution, name)\n",
    "    print(juicer_loop, file=fp)\n",
    "    fp.close()\n",
    "    \n",
    "# TADLib\n",
    "def preparing_TADLib_file(h3dgipdir, filename, resolution):\n",
    "    matrix = '../HiCPro_result/hic_results/matrix/raw/{0}/{1}_{0}.matrix'.format(resolution, filename)\n",
    "    bins = '../HiCPro_result/hic_results/matrix/raw/{0}/{1}_{0}_abs.bed'.format(resolution, filename)\n",
    "    output_file = 'TADLib/run_TADLib.sh'\n",
    "    fp = open(output_file, \"w\")\n",
    "    # 1、split chr\n",
    "    split_chr_order = \"python {0}/utils/hicproTocool.py -m {1} -b {2} -o TADLib/intramtx -YN N\".format(h3dgipdir, matrix, bins)\n",
    "    print(split_chr_order, file=fp)\n",
    "    # 2、to cool\n",
    "    dataset_file = 'TADLib/dataset'\n",
    "    fd = open(dataset_file, 'w')\n",
    "    print(\"res:{}\".format(resolution), file=fd)\n",
    "    print(\" ./intramtx\", file=fd)\n",
    "    fd.close()\n",
    "    tocool_order = \"toCooler -O TADLib/TADLib.cool -d dataset --chromsize-file ../HiCPro/HiC-Pro_chromosome_size.txt\"\n",
    "    print(tocool_order, file=fp)\n",
    "    # 3、find tad\n",
    "    meta_file = 'TADLib/meta_file'\n",
    "    fm = open(meta_file, 'w')\n",
    "    print(\"res:{}\".format(resolution), file=fm)\n",
    "    print(\" rep1:TADLib/TADLib.cool::{}\".format(resolution), file=fm)\n",
    "    fm.close()\n",
    "    find_TAD_order = \"hitad -O TADLib_result/TADLib_TAD.bed -d meta_file --logFile hitad.log -p 4\"\n",
    "    print(find_TAD_order, file=fp)\n",
    "    fp.close()\n",
    "\n",
    "# FitHiC \n",
    "def preparing_FitHiC_file(h3dgipdir, filename, resolution):\n",
    "    matrix = '../HiCPro_result/hic_results/matrix/raw/{0}/{1}_{0}.matrix'.format(resolution, filename)\n",
    "    bins = '../HiCPro_result/hic_results/matrix/raw/{0}/{1}_{0}_abs.bed'.format(resolution, filename)\n",
    "    output_file = 'FitHiC/run_FitHiC.sh'\n",
    "    fp = open(output_file, \"w\")\n",
    "    fithic_step1 = \"python {0}/utils/hicpro2fithic.py -i {1} -b {2} -r {3} -o ./ -n FitHiC\".format(h3dgipdir, matrix, bins, resolution)\n",
    "    fithic_step2 = \"fithic -f ./FitHiC.fithic.fragmentMappability.gz -i ./FitHiC.fithic.interactionCounts.gz -r {} -L 6000 -U 3000000 -p 2 -o FitHiC_result -l FitHiC\".format(resolution) # 3 Kb resolution\n",
    "    print(fithic_step1, file=fp)\n",
    "    print(fithic_step2, file=fp)\n",
    "    fp.close()\n",
    "\n",
    "# hichipper\n",
    "def preparing_hichipper_file():\n",
    "    # build the config file of hichipper\n",
    "    import os\n",
    "    workDir = os.getcwd() \n",
    "    fy = open('./hichipper/hichipper_config.yaml', 'w')\n",
    "    print(\"peaks:\", file=fy)\n",
    "    print(\"  - COMBINED,ALL\", file=fy)\n",
    "    print(\"resfrags:\", file=fy)\n",
    "    print(\"  - {0}/HiCPro/HiC-Pro_enzyme_fragments.bed\".format(workDir), file=fy)\n",
    "    print(\"hicpro_output:\", file=fy)\n",
    "    print(\"  - {0}/HiCPro_result/hic_reuslts\".format(workDir), file=fy)\n",
    "    fy.close()\n",
    "    # build the run file of hichipper\n",
    "    output_file = \"./hichipper/run_hichipper.sh\"\n",
    "    fp = open(output_file, \"w\")\n",
    "    hichipper_order = \"hichipper --out hichipper_result hichipper_config.yaml\"\n",
    "    print(hichipper_order, file=fp)\n",
    "    fp.close()\n",
    "\n",
    "# FitHiChIP\n",
    "def preparing_FitHiChIP_file(h3dgipdir, name):\n",
    "    output_file = \"./FitHiChIP/run_FitHiChIP.sh\"\n",
    "    fp = open(output_file, \"w\")\n",
    "    # configure file\n",
    "    cp_order = \"cp {}/utils/configfile_BiasCorrection_ICEBias ./\".format(h3dgipdir)\n",
    "    sed_order1 = \"sed -i 's/ValidPairs/..\\/HiCPro\\/hic_results\\/data\\/{0}\\/{0}.allValidPairs/g' configfile_BiasCorrection_ICEBias\".format(name)\n",
    "    sed_order2 = \"sed -i 's/chromSize/..\\/HiCPro\\/HiC-Pro_chromosome_size.txt/g' configfile_BiasCorrection_ICEBias\"\n",
    "    print(cp_order, file=fp)\n",
    "    print(sed_order1, file=fp)\n",
    "    print(sed_order2, file=fp)\n",
    "    FitHiChIP_order = \"bash {0}/utils/FitHiChIP_HiCPro.sh -C configfile_BiasCorrection_ICEBias\".format(h3dgipdir)\n",
    "    print(FitHiChIP_order, file=fp)\n",
    "    print(\"# The optional compilation file contains configfile_BiasCorrection_ICEBias, configfile_BiasCorrection_CoverageBias, configfile_P2P_BiasCorrection_CoverageBias, configfile_P2P_BiasCorrection_ICEBias\", file=fp)\n",
    "    fp.close()\n",
    "\n",
    "# creat directory\n",
    "def preparing_directory():    \n",
    "    def make_dir(dirname):\n",
    "        import os\n",
    "        work_path = os.getcwd()\n",
    "        file_path = \"{0}/{1}\".format(work_path, dirname)\n",
    "        print(file_path)\n",
    "        if os.path.exists(file_path):\n",
    "            print('The folder exists')\n",
    "        else:\n",
    "            os.makedirs(file_path)\n",
    "    all_dir = ['HiCPro', 'TADLib', 'FitHiC', 'hichipper', 'FitHiChIP', 'juicer']\n",
    "    for d in all_dir:\n",
    "        make_dir(d)\n",
    "\n",
    "\n",
    "# build workflows\n",
    "def H3DGIP_workflows(h3dgipdir, input_dir, genome, enzyme, compartment_resolution, tad_resolution, loop_resolution, name):\n",
    "    #make directory\n",
    "    preparing_directory()\n",
    "    # HiC-Pro\n",
    "    preparing_HiCPro_file(h3dgipdir, genome, enzyme, input_dir)\n",
    "    # Juicer\n",
    "    preparing_juicer_file(h3dgipdir, genome, enzyme, input_dir, compartment_resolution, tad_resolution, loop_resolution, name)\n",
    "    # TADLib\n",
    "    preparing_TADLib_file(h3dgipdir, name, tad_resolution)\n",
    "    # FitHiC \n",
    "    preparing_FitHiC_file(h3dgipdir, name, loop_resolution)\n",
    "    # hichipper\n",
    "    preparing_hichipper_file()\n",
    "    # FitHiChIP\n",
    "    preparing_FitHiChIP_file(h3dgipdir, name)\n",
    "    # write the pipeline\n",
    "    fp = open(\"H3DGIP_pipeline.sh\", 'w')\n",
    "    for i in ['HiCPro', 'juicer', 'TADLib', 'FitHiC', 'FitHiChIP', 'hichipper']:\n",
    "        print(\"#{}\".format(i), file=fp)\n",
    "        print(\"cd {}\".format(i), file=fp)\n",
    "        print(\"sh run_{}.sh\".format(i), file=fp)\n",
    "        print(\"cd ..\", file=fp)\n",
    "        print(\"\", file=fp)\n",
    "    fp.close()\n",
    "\n",
    "def main(argv):\n",
    "    import sys\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"\")\n",
    "    parser.add_argument(\"-d\", \"--h3dgip_directory\", required=True, help=\"The directory of H3DGIP\")\n",
    "    parser.add_argument('-i', '--input_directory', required=True, help=\"The directory of input file\")\n",
    "    parser.add_argument('-n', '--sample_name', required=True, help=\"Sample name\")\n",
    "    parser.add_argument('-g', '--genome', required=True, help=\"The genome file\")\n",
    "    parser.add_argument('-e', '--enzyme', required=True, help=\"The type of restriction enzyme\")\n",
    "    parser.add_argument('-c', '--compartment_resolution', required=True, help=\"The resolution of compartment\")\n",
    "    parser.add_argument('-t', '--tad_resolution', required=True, help='The resolution of TAD')\n",
    "    parser.add_argument('-l', '--loop_resolution', required=True, help='The resolution of loop')\n",
    "    args = parser.parse_args()\n",
    "    h3dgipdir, input_dir, name, genome, enzyme, compartment_resolution, tad_resolution, loop_resolution  = args.h3dgip_directory, args.input_directory, args.sample_name, args.genome, args.enzyme, args.compartment_resolution, args.tad_resolution, args.loop_resolution\n",
    "    H3DGIP_workflows(h3dgipdir, input_dir, genome, enzyme, compartment_resolution, tad_resolution, loop_resolution, name)\n",
    "import sys\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import argparse\n",
    "def main(argv):\n",
    "    parser = argparse.ArgumentParser(description=\"\")\n",
    "    parser.add_argument('-b', \"--bedfile\", required=True, help=\"The bed file generated by HiC-Pro\")\n",
    "    parser.add_argument('-m', '--matrixfile', required=True, help=\"Matrix file generated by HiC-Pro\")\n",
    "    parser.add_argument('-o', \"--outdir\", required=True, help = 'Output file directory')\n",
    "    parser.add_argument('-YN', '--Scaffold', required=True, help=\"Is scaffold internal interaction required\")\n",
    "    args = parser.parse_args()\n",
    "    bed_f = args.bedfile\n",
    "    mtx_f = args.matrixfile\n",
    "    out_dir = args.outdir\n",
    "    is_scaf = args.Scaffold\n",
    "    df_bed = pd.read_csv(bed_f, sep='\\t', names=['chr', 's', 'e', 'id'])\n",
    "    df_mtx = pd.read_csv(mtx_f, sep='\\t', names=['bin1', 'bin2', 'reads'])\n",
    "    #Determine if the scaffold id need to be remove\n",
    "    if is_scaf == 'NO' or is_scaf == 'no' or is_scaf == 'N':\n",
    "        df_bed = df_bed.query(\"~chr.str.contains('Scaff')\", engine='python').copy()\n",
    "        #df_bed = df_bed.query(\"~chr.str.contains('Contig')\", engine='python').copy()\n",
    "    # get need chr id\n",
    "    chr_id = sorted(set(list(df_bed['chr'])))\n",
    "    #print(chr_id)\n",
    "    n = 1\n",
    "    for c in chr_id:\n",
    "        names = \"{0}_{0}.txt\".format(n)\n",
    "        out_f1 = os.path.join(out_dir, names)\n",
    "        out_f2 = os.path.join(out_dir, c+\"_abs.bed\")\n",
    "        df_chr = df_bed.query(\"chr == @c\").copy()\n",
    "        id_start = df_chr['id'].min()\n",
    "        id_end = df_chr['id'].max()\n",
    "        df_chr_mtx = df_mtx.query(\"(bin1 >= @id_start) & (bin1 <= @id_end) & (bin2 >= @id_start) & (bin2 <= @id_end)\").copy()\n",
    "        df_chr_mtx['bin1'] = df_chr_mtx['bin1'] - id_start + 1\n",
    "        df_chr_mtx['bin2'] = df_chr_mtx['bin2'] - id_start + 1\n",
    "        df_chr['id'] = df_chr['id'] - id_start + 1\n",
    "        df_chr_mtx.to_csv(out_f1, sep='\\t', index=False, header=False)\n",
    "        df_chr.to_csv(out_f2, sep='\\t', index=False, header=False)\n",
    "        n += 1\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "RE_cutsite = {\n",
    "    \"mboi\": [\"^GATC\"],\n",
    "    \"dpnii\": [\"^GATC\"],\n",
    "    \"bglii\": [\"A^GATCT\"],\n",
    "    \"hindiii\": [\"A^AGCTT\"]}\n",
    "\n",
    "\n",
    "def find_re_sites(filename, sequences, offset):\n",
    "    with open(filename, 'r') as infile:\n",
    "        chr_id = None\n",
    "        big_str = \"\"\n",
    "        indices = []\n",
    "        all_indices = []\n",
    "        contig_names = []\n",
    "        c = 0\n",
    "        for line in infile:\n",
    "            c += 1\n",
    "            if line.startswith(\">\"):\n",
    "                print(\"{}...\".format(line.split()[0][1:]))\n",
    "                # If this is not the first chromosome, find the indices and append\n",
    "                # them to the list\n",
    "                if chr_id is not None:\n",
    "                     for rs in range(len(sequences)):\n",
    "                         pattern = \"(?={})\".format(sequences[rs].lower())\n",
    "                         indices += [m.start() + offset[rs]\\\n",
    "                         for m in re.finditer(pattern, big_str)]\n",
    "                     indices.sort()\n",
    "                     all_indices.append(indices)\n",
    "                     indices = []\n",
    "\n",
    "                # This is a new chromosome. Empty the sequence string, and add the\n",
    "                # correct chrom id\n",
    "                big_str = \"\"\n",
    "                chr_id = line.split()[0][1:]\n",
    "                if chr_id in contig_names:\n",
    "                    print(\"The fasta file contains several instance of {}. Exit.\".format(chr_id))\n",
    "                    sys.exit(-1)\n",
    "                contig_names.append(chr_id)\n",
    "            else:\n",
    "                # As long as we don't change chromosomes, continue reading the\n",
    "                # file, and appending the sequences\n",
    "                big_str += line.lower().strip()\n",
    "        # Add the indices for the last chromosome\n",
    "        for rs in range(len(sequences)):\n",
    "            pattern = \"(?={})\".format(sequences[rs].lower())\n",
    "            indices += [m.start() + offset[rs]\n",
    "                        for m in re.finditer(pattern, big_str)]\n",
    "        indices.sort()\n",
    "        all_indices.append(indices)\n",
    "    \n",
    "    return contig_names, all_indices\n",
    "\n",
    "\n",
    "def find_chromsomose_lengths(reference_filename):\n",
    "    chromosome_lengths = []\n",
    "    chromosome_names = []\n",
    "    length = None\n",
    "    with open(reference_filename, 'r') as infile:\n",
    "        for line in infile:\n",
    "            if line.startswith(\">\"):\n",
    "                chromosome_names.append(line[1:].strip())\n",
    "                if length is not None:\n",
    "                    chromosome_lengths.append(length)\n",
    "                length = 0\n",
    "            else:\n",
    "                length += len(line.strip())\n",
    "        chromosome_lengths.append(length)\n",
    "    return chromosome_names, np.array(chromosome_lengths)\n",
    "\n",
    "\n",
    "def replaceN(cs):\n",
    "    npos = int(cs.find('N'))\n",
    "    cseql = []\n",
    "    if npos != -1:\n",
    "        for nuc in [\"A\",\"C\",\"G\",\"T\"]:\n",
    "            tmp = cs.replace('N', nuc, 1)\n",
    "            tmpl = replaceN(tmp)\n",
    "            if type(tmpl) == list:\n",
    "                cseql = cseql + tmpl\n",
    "            else:\n",
    "                cseql.append(tmpl)\n",
    "    else:\n",
    "        cseql.append(cs)\n",
    "    return cseql\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('fastafile')\n",
    "    parser.add_argument('-r', '--restriction_sites',\n",
    "                        dest='res_sites',\n",
    "                        nargs='+',\n",
    "                        help=(\"The cutting position has to be specified using \"\n",
    "                              \"'^'. For instance, -r A^AGCTT for HindIII \"\n",
    "                              \"digestion. Several restriction enzyme can be \"\n",
    "                              \"specified.\"))\n",
    "    parser.add_argument('-o', '--out', default=None)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    filename = args.fastafile\n",
    "    out = args.out\n",
    "    \n",
    "    # Split restriction sites if comma-separated\n",
    "    cutsites=[]\n",
    "    for s in args.res_sites:\n",
    "        for m in s.split(','):\n",
    "            cutsites.append(m)\n",
    "                \n",
    "    # process args and get restriction enzyme sequences\n",
    "    sequences = []\n",
    "    offset = []\n",
    "    for cs in cutsites:\n",
    "        if cs.lower() in RE_cutsite:\n",
    "            cseq = ''.join(RE_cutsite[cs.lower()])\n",
    "        else:\n",
    "            cseq = cs\n",
    "\n",
    "        offpos = int(cseq.find('^'))\n",
    "        if offpos == -1:\n",
    "            print(\"Unable to detect offset for {}. Please, use '^' to specify the cutting position,\\\n",
    "                   i.e A^GATCT for HindIII digestion.\".format(cseq))\n",
    "            sys.exit(-1)\n",
    "\n",
    "        for nuc in list(set(cseq)):\n",
    "            if nuc not in ['A','T','G','C','N','^']:\n",
    "                print(\"Find unexpected character ['{}']in restriction motif\".format(nuc))\n",
    "                print(\"Note that multiple motifs should be separated by a space (not a comma !)\")\n",
    "\n",
    "                sys.exit(-1)\n",
    "\n",
    "        offset.append(offpos)\n",
    "        sequences.append(re.sub('\\^', '', cseq))\n",
    "\n",
    "    # replace all N in restriction motif\n",
    "    sequences_without_N = []\n",
    "    offset_without_N = []\n",
    "    for rs in range(len(sequences)):\n",
    "        nrs = replaceN(sequences[rs])\n",
    "        sequences_without_N = sequences_without_N + nrs\n",
    "        offset_without_N = offset_without_N + [offset[rs]] * len(nrs)\n",
    "          \n",
    "    sequences = sequences_without_N\n",
    "    offset = offset_without_N\n",
    "    \n",
    "    if out is None:\n",
    "        out = os.path.splitext(filename)[0] + \"_fragments.bed\"\n",
    "\n",
    "    print(\"Analyzing\", filename)\n",
    "    print(\"Restriction site(s)\", \",\".join(sequences))\n",
    "    print(\"Offset(s)\",  ','.join(str(x) for x in offset))\n",
    "\n",
    "    # Read fasta file and look for rs per chromosome\n",
    "    contig_names, all_indices = find_re_sites(filename, sequences,  offset=offset)\n",
    "    _, lengths = find_chromsomose_lengths(filename)\n",
    "\n",
    "    valid_fragments = []\n",
    "    for i, indices in enumerate(all_indices):\n",
    "        valid_fragments_chr = np.concatenate(\n",
    "            [np.concatenate([[0], indices])[:, np.newaxis],\n",
    "             np.concatenate([indices, [lengths[i]]])[:, np.newaxis]],\n",
    "            axis=1)\n",
    "        valid_fragments.append(valid_fragments_chr)\n",
    "\n",
    "    # Write results\n",
    "    print(\"Writing to {} ...\".format(out))\n",
    "    with open(out, 'w') as outfile:\n",
    "        for chrom_name, indices in zip(contig_names, valid_fragments):\n",
    "            frag_id = 0\n",
    "            for begin, end in indices:\n",
    "                # allow to remove cases where the enzyme cut at\n",
    "                # the first position of the chromosome\n",
    "                if end > begin:\n",
    "                    frag_id += 1\n",
    "                    frag_name = \"HIC_{}_{}\".format(str(chrom_name), int(frag_id))\n",
    "                    outfile.write(\"{}\\t{}\\t{}\\t{}\\t0\\t+\\n\".format(str(chrom_name), int(begin), int(end), str(frag_name)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0f669ea42a4dcea88d385f8f9b4c3d9b966e24234758831d1dbfdac925619a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
